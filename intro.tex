\begin{frame}
\frametitle{Introduction}
\begin{itemize}
\item<1-> The bandit problem is a sequential decision making process where at each timestep we have to choose one action or arm from a set of arms.
\item<2-> There is a specific reward distribution attached to each arm.  After pulling an arm we receive a reward from the reward distribution specific to the arm. 
\item<3-> After say pulling each arm once, we are presented with an \emph{exploration-exploitation}  trade-off, that is whether to continue to pull the arm for which we have observed the highest estimated reward till now(exploitation) or to explore a new arm(exploration). 
\item<4-> If we become too greedy and always exploit, we may miss the chance of actually finding the optimal arm and get stuck with a sub-optimal arm.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Some practical applications}
\begin{itemize}
\item<1-> Selecting the best channel (out of several existing channels) for mobile communications in a very short duration.
\item<2-> Selecting a small set of best workers (out of a very large pool of workers) whose productivity is above a threshold.
\item<3-> Selecting the best possible route for a message to pass through in a peer-to-peer network connection.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why study bandits at all?}
\begin{itemize}
\item<1-> We know of $\epsilon$-greedy \cite{sutton1998reinforcement} algorithm, we can simply stick to it.
\item<2-> But $\epsilon$-greedy only gives us an asymptotic guarantee. There is no guarantee that in a highly regressive environment how $\epsilon$-greedy will behave. Can we be better in our search?
\item<3-> Bandits allows us to study this behavior in a more formal way giving us strict guarantees regarding the performance of our algorithm.
%\item<4-> They form the linking pieces of a larger problem.
\item<4-> They are easy to implement.    
\end{itemize}
\end{frame}

