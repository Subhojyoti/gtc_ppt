\begin{frame}
\frametitle{Stochastic Multi-Armed Bandit Problem (SMAB)}
\begin{itemize}
\item<1-> The thresholding bandit problem falls under the broad area of stochastic multi-armed bandit problem.
\item<2-> A finite set of actions or arms belonging to set $A$ such that $|A|=K$. 
\item<3-> The rewards for each of the arms are identical and independent random variables drawn from distribution specific to the arm.
\item<4-> The learner does not know the mean $r_{i},\forall i\in A$ of the distribution or the variance $\sigma_i^2$. 
%\item<5-> The distributions for each of the arms are fixed throughout the time horizon denoted by $T$. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Stochastic Multi-Armed Bandit Problem (SMAB)}
\begin{itemize}
\item<1-> The distributions for each of the arms are fixed throughout the time horizon denoted by $T$. 
\item<2-> The estimated reward $\hat{r}_{i}=\frac{1}{n_{i}}\sum_{z=1}^{n_i} X_{i,z}$.
\item<3-> The more we pull arm $i$ the closer $\hat{r}_i$ gets to $r_i$.
\item<4-> Due to the uncertainty in $\hat{r}_i$ we have carefully conduct exploration.
\end{itemize}
\end{frame}


